{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "c60440e3-fa8f-4256-8933-b673f6568f0a",
   "metadata": {
    "tags": []
   },
   "source": [
    "# Bag of Words Meets Bags of Popcorn\n",
    "\n",
    "Notebook for optimized performance."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ce714418-69e5-4609-9231-175fe0ec6a34",
   "metadata": {
    "tags": []
   },
   "source": [
    "## Module Imports"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "88b4cc21-9599-4284-9163-efdb2430bd92",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import re\n",
    "from collections import Counter\n",
    "from typing import *\n",
    "\n",
    "import nltk\n",
    "import pandas as pd\n",
    "from bs4 import BeautifulSoup\n",
    "from gensim.test.utils import common_texts, get_tmpfile\n",
    "from gensim.models.doc2vec import Doc2Vec, TaggedDocument\n",
    "from nltk.corpus import stopwords\n",
    "from nltk.stem.snowball import SnowballStemmer\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.metrics import classification_report\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.utils import shuffle\n",
    "from tqdm import tqdm"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b06e77ed-d5a3-4ecf-ad3c-3a9c9c6bdfa5",
   "metadata": {},
   "source": [
    "## Configs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "ea95d32e-6458-4cb7-828c-d1da9d228150",
   "metadata": {},
   "outputs": [],
   "source": [
    "DATA_DIR = '../data/word2vec-nlp-tutorial'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "8d256a7c-b10b-4c5e-bce6-497808912d3a",
   "metadata": {},
   "outputs": [],
   "source": [
    "nltk.data.path.append('/Users/chenjun/workspace/nltk_data')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0fbc4644-8e56-499f-b3ec-889ff364c751",
   "metadata": {},
   "source": [
    "## EDA"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "1f270554-bbd0-44f3-8c2f-8e3881ef9920",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['.DS_Store',\n",
       " 'sampleSubmission.csv',\n",
       " 'labeledTrainData.tsv',\n",
       " 'testData.tsv',\n",
       " 'unlabeledTrainData.tsv']"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "os.listdir(DATA_DIR)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "8b3c3b43-5d43-48ab-b474-3a928e05280c",
   "metadata": {},
   "outputs": [],
   "source": [
    "unlabeled_train_data = pd.read_csv(os.path.join(DATA_DIR, 'unlabeledTrainData.tsv'), header=0, delimiter='\\t', quoting=3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "80ef0e8d-b878-4aa3-a384-7af037f1d728",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(50000, 2)\n",
      "          id                                             review\n",
      "0   \"9999_0\"  \"Watching Time Chasers, it obvious that it was...\n",
      "1  \"45057_0\"  \"I saw this film about 20 years ago and rememb...\n",
      "2  \"15561_0\"  \"Minor Spoilers<br /><br />In New York, Joan B...\n",
      "3   \"7161_0\"  \"I went to see this film with a great deal of ...\n",
      "4  \"43971_0\"  \"Yes, I agree with everyone on this site this ...\n"
     ]
    }
   ],
   "source": [
    "print(unlabeled_train_data.shape)\n",
    "print(unlabeled_train_data.head())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "104fd903-9f86-4b29-a0ac-6f76a098fc2b",
   "metadata": {},
   "outputs": [],
   "source": [
    "labeled_train_data = pd.read_csv(os.path.join(DATA_DIR, 'labeledTrainData.tsv'), header=0, delimiter='\\t', quoting=3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "28b21a6a-0ab0-4fa9-a308-64cb2789408c",
   "metadata": {},
   "outputs": [],
   "source": [
    "labeled_train_data = shuffle(labeled_train_data)\n",
    "labeled_train_data.reset_index(inplace=True, drop=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "0de4afe7-7737-496a-8869-d054d8ac7894",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Counter({1: 12500, 0: 12500})\n"
     ]
    }
   ],
   "source": [
    "print(Counter(labeled_train_data['sentiment'].tolist()))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "4843cec5-1d14-40de-8714-b73cd7358fe7",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(25000, 3)\n",
      "          id  sentiment                                             review\n",
      "0    \"681_9\"          1  \"Richard Chamberlain is David Burton, a tax la...\n",
      "1  \"11797_8\"          1  \"Powers Booth is hypnotic as cult leader jim j...\n",
      "2   \"7722_3\"          0  \"Extremely poor action film starring the ever ...\n",
      "3   \"5245_8\"          1  \"I thought this movie was LOL funny. It's a fu...\n",
      "4   \"7302_3\"          0  \"The first five minutes of this movie showed p...\n"
     ]
    }
   ],
   "source": [
    "print(labeled_train_data.shape)\n",
    "print(labeled_train_data.head())"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a9c1b660-6149-4c4f-a59f-590adfadf0b2",
   "metadata": {},
   "source": [
    "## Data Preprocessing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "25ce1f04-30a8-4c22-bf04-e08ca92c3eca",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of stop words loaded: 179\n"
     ]
    }
   ],
   "source": [
    "STOP_WORDS = set(stopwords.words('english'))\n",
    "print(f'Number of stop words loaded: {len(STOP_WORDS)}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "49ff2c9f-9785-42bd-85a8-1deec1c7b683",
   "metadata": {},
   "outputs": [],
   "source": [
    "stemmer = SnowballStemmer('english')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "adb857f4-ab14-4da6-8dc9-0e5857a78303",
   "metadata": {},
   "outputs": [],
   "source": [
    "def clean_review(raw_review: str, remove_stop_words: bool = True, stem: bool = True) -> List[str]:\n",
    "    \"\"\"Data cleanning\"\"\"\n",
    "    \n",
    "    # Remove HTML markups\n",
    "    cleaned_review = BeautifulSoup(raw_review).get_text()\n",
    "    \n",
    "    # Remove punctuations\n",
    "    cleaned_review = re.sub('[^a-zA-Z]', ' ', cleaned_review)\n",
    "    \n",
    "    # Lower and split words\n",
    "    words = cleaned_review.lower().split()\n",
    "    \n",
    "    # Remove stop wrods\n",
    "    if remove_stop_words:\n",
    "        words = [word for word in words if word not in STOP_WORDS]\n",
    "        \n",
    "    # Stem\n",
    "    if stem:\n",
    "        words = [stemmer.stem(word) for word in words]\n",
    "    \n",
    "    return words"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "fbba5c01-73d6-4a59-b768-8087f9687a4a",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['watch', 'time', 'chaser', 'it', 'obvious', 'that', 'it', 'was', 'made', 'by', 'a', 'bunch', 'of', 'friend', 'mayb', 'they', 'were', 'sit', 'around', 'one', 'day', 'in', 'film', 'school', 'and', 'said', 'hey', 'let', 's', 'pool', 'our', 'money', 'togeth', 'and', 'make', 'a', 'realli', 'bad', 'movi', 'or', 'someth', 'like', 'that', 'what', 'ever', 'they', 'said', 'they', 'still', 'end', 'up', 'make', 'a', 'realli', 'bad', 'movi', 'dull', 'stori', 'bad', 'script', 'lame', 'act', 'poor', 'cinematographi', 'bottom', 'of', 'the', 'barrel', 'stock', 'music', 'etc', 'all', 'corner', 'were', 'cut', 'except', 'the', 'one', 'that', 'would', 'have', 'prevent', 'this', 'film', 's', 'releas', 'life', 's', 'like', 'that']\n"
     ]
    }
   ],
   "source": [
    "print(clean_review(unlabeled_train_data['review'][0], remove_stop_words=False, stem=True))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "db00c968-d77a-453e-8828-6d3e050c2b03",
   "metadata": {},
   "outputs": [],
   "source": [
    "def preprocess(raw_df: pd.DataFrame, remove_stop_words: bool = True, stem: bool = True) -> List[List[str]]:\n",
    "    \"\"\"Data Preprocessing\n",
    "    \n",
    "    Map a textual review to a list of words.\n",
    "    \"\"\"\n",
    "    data = []\n",
    "    for idx in tqdm(range(raw_df.shape[0])):\n",
    "        row = clean_review(raw_df['review'][idx], remove_stop_words=remove_stop_words, stem=stem)\n",
    "        data.append(row)\n",
    "        \n",
    "    return data"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ce39334a-f961-499e-9b62-f741f0147fc1",
   "metadata": {},
   "source": [
    "## Train Doc2Vec Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "cf8b1870-eb2e-4f6f-a164-e940581eedba",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  2%|██▌                                                                                                                                                                 | 781/50000 [00:01<01:05, 751.31it/s]/Users/chenjun/opt/miniconda3/envs/py38_tf29/lib/python3.8/site-packages/bs4/__init__.py:435: MarkupResemblesLocatorWarning: The input looks more like a filename than markup. You may want to open this file and pass the filehandle into Beautiful Soup.\n",
      "  warnings.warn(\n",
      "100%|██████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████| 50000/50000 [01:05<00:00, 761.28it/s]\n",
      "100%|███████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████| 50000/50000 [00:00<00:00, 321924.59it/s]\n"
     ]
    }
   ],
   "source": [
    "# Prepare training data\n",
    "tagged_documents = []\n",
    "docs = preprocess(unlabeled_train_data, remove_stop_words=False, stem=True)\n",
    "for idx in tqdm(range(len(docs))):\n",
    "    tagged_documents.append(TaggedDocument(docs[idx], [idx]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "cd01171b-750b-43c4-b245-50220de8554c",
   "metadata": {},
   "outputs": [],
   "source": [
    "d2v_model = Doc2Vec(tagged_documents, vector_size=50, window=2, min_count=1, workers=6)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "97c4079c-2ca9-4776-a06c-a72e5de972a1",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(50,)\n"
     ]
    }
   ],
   "source": [
    "print(d2v_model.infer_vector([\"all\", \"stuff\"]).shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "3e6369e7-2044-49f8-8d5d-825562112ec0",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Trained Doc2Vec model saved to: /Users/chenjun/workspace/InitialJ/Kaggle/word2vec-nlp-tutorial/d2v_model\n"
     ]
    }
   ],
   "source": [
    "# Save \n",
    "d2v_model_path = os.path.abspath(os.path.join(os.path.curdir, 'd2v_model'))  # This should be absolute path.\n",
    "fname = get_tmpfile(d2v_model_path)\n",
    "d2v_model.save(fname)\n",
    "print(f'Trained Doc2Vec model saved to: {d2v_model_path}')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "acbd50d9-b3a3-416c-ac38-8ced1162f8d3",
   "metadata": {},
   "source": [
    "## Feature Engineering"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "3bf865a5-6656-4699-8f0f-635dca558dde",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(25000, 3)\n",
      "          id  sentiment                                             review\n",
      "0    \"681_9\"          1  \"Richard Chamberlain is David Burton, a tax la...\n",
      "1  \"11797_8\"          1  \"Powers Booth is hypnotic as cult leader jim j...\n",
      "2   \"7722_3\"          0  \"Extremely poor action film starring the ever ...\n",
      "3   \"5245_8\"          1  \"I thought this movie was LOL funny. It's a fu...\n",
      "4   \"7302_3\"          0  \"The first five minutes of this movie showed p...\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "(None, None)"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "print(labeled_train_data.shape), print(labeled_train_data.head())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "26ea92ec-5e13-4e7b-bc3b-e495b46ea087",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/chenjun/opt/miniconda3/envs/py38_tf29/lib/python3.8/site-packages/bs4/__init__.py:435: MarkupResemblesLocatorWarning: The input looks more like a filename than markup. You may want to open this file and pass the filehandle into Beautiful Soup.\n",
      "  warnings.warn(\n"
     ]
    }
   ],
   "source": [
    "train_data_features = []\n",
    "for row in labeled_train_data.itertuples():\n",
    "    doc = clean_review(row.review, remove_stop_words=False, stem=True)\n",
    "    train_data_features.append(d2v_model.infer_vector(doc))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "2ada784d-3429-4ded-a4a4-be581f6c7730",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "25000 50\n"
     ]
    }
   ],
   "source": [
    "print(len(train_data_features), len(train_data_features[0]))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0ce13b6f-6fac-4053-8f72-d5d62aad8c3b",
   "metadata": {
    "tags": []
   },
   "source": [
    "## Model\n",
    "\n",
    "Various models."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "94cdb874-9091-456a-aa51-f92b2f39967b",
   "metadata": {},
   "source": [
    "### Random Forest"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "9b229a75-f2de-4a65-bc29-f35d2920c3a5",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Train-test Split\n",
    "split_ratio = 0.3\n",
    "X_train, X_test, y_train, y_test = train_test_split(\n",
    "    train_data_features, \n",
    "    labeled_train_data['sentiment'], \n",
    "    test_size=split_ratio, \n",
    "    random_state=2\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "75d9ab4f-8548-422a-8659-39b65d2b4d67",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Initialize a Random Forest classifier with 100 trees\n",
    "forest = RandomForestClassifier(n_estimators=100, n_jobs=-1)\n",
    "\n",
    "# Fit the forest to the training set\n",
    "forest = forest.fit(X_train, y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "771eb98a-cf1b-49b9-8cd9-ff3181eda153",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "1.0"
      ]
     },
     "execution_count": 25,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "forest.score(X_train, y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "c34cba01-3bdf-4bab-b251-0d9a4364821b",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.8004"
      ]
     },
     "execution_count": 26,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "forest.score(X_test, y_test)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ddc453a1-a8d7-4021-a21d-660e3d0efc27",
   "metadata": {},
   "source": [
    "### Logistic Regression"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "7592e3d0-7d99-4169-901b-ccea51a24b6e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.8264\n",
      "0.8210666666666666\n"
     ]
    }
   ],
   "source": [
    "logistic_regression_model = LogisticRegression(max_iter=10000, n_jobs=-1)\n",
    "logistic_regression_model.fit(X_train, y_train)\n",
    "print(logistic_regression_model.score(X_train, y_train))\n",
    "print(logistic_regression_model.score(X_test, y_test))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "552e600d-4e57-4a1d-ba95-662ba0c5826c",
   "metadata": {},
   "source": [
    "## Evaluate"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "19a7b70c-135d-484d-afc9-9c5d0ca3d42f",
   "metadata": {},
   "outputs": [],
   "source": [
    "def evaluate(model, features, labels):\n",
    "    predictions = model.predict(features)\n",
    "    \n",
    "    report = classification_report(predictions, labels)\n",
    "    \n",
    "    return report"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "93ea539b-8f67-4423-b767-79a8e5cf1fdb",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.79      0.80      0.80      3691\n",
      "           1       0.81      0.80      0.80      3809\n",
      "\n",
      "    accuracy                           0.80      7500\n",
      "   macro avg       0.80      0.80      0.80      7500\n",
      "weighted avg       0.80      0.80      0.80      7500\n",
      "\n"
     ]
    }
   ],
   "source": [
    "print(evaluate(forest, X_test, y_test))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "94dd5e22-922a-428d-95eb-423f86be649e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.82      0.82      0.82      3694\n",
      "           1       0.83      0.82      0.82      3806\n",
      "\n",
      "    accuracy                           0.82      7500\n",
      "   macro avg       0.82      0.82      0.82      7500\n",
      "weighted avg       0.82      0.82      0.82      7500\n",
      "\n"
     ]
    }
   ],
   "source": [
    "print(evaluate(logistic_regression_model, X_test, y_test))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1764efbb-db74-4abc-850c-49cd3dc8e70f",
   "metadata": {},
   "source": [
    "## Create Submission"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "3bcc2866-b0cd-427f-bd67-a1d73c9e59b0",
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_submission(sub_name: str):\n",
    "    # Read the test data\n",
    "    test = pd.read_csv(os.path.join(DATA_DIR, \"testData.tsv\"), header=0, delimiter=\"\\t\", quoting=3)\n",
    "\n",
    "    # Create an empty list and append the clean reviews one by one\n",
    "    num_reviews = len(test[\"review\"])\n",
    "    clean_test_reviews = [] \n",
    "\n",
    "    for i in tqdm(range(0,num_reviews)):\n",
    "        cleaned_review = clean_review(test[\"review\"][i])\n",
    "        clean_test_reviews.append(cleaned_review)\n",
    "\n",
    "    # Get a bag of words for the test set, and convert to a numpy array\n",
    "    test_data_features = list(map(lambda doc: d2v_model.infer_vector(doc), clean_test_reviews))\n",
    "\n",
    "    # Use the random forest to make sentiment label predictions\n",
    "    result = forest.predict(test_data_features)\n",
    "\n",
    "    # Copy the results to a pandas dataframe with an \"id\" column and\n",
    "    # a \"sentiment\" column\n",
    "    output = pd.DataFrame(data={\"id\":test[\"id\"], \"sentiment\":result})\n",
    "\n",
    "    # Use pandas to write the comma-separated output file\n",
    "    output.to_csv(sub_name, index=False, quoting=3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "id": "71622676-0d75-41f2-ae2d-4fa08f7858f6",
   "metadata": {},
   "outputs": [],
   "source": [
    "# create_submission(\"Bag_of_Words_model_v3.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8e6b342b-26e5-404a-adeb-42b21b5f5c22",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
